# 1/2/3 後ろ向き微分計算 backward


公式bookの1/2/3




backward()とは何か？
深層学習の目的は、なんらかのモデルFに対して、その誤差関数Lを最小化することでした。


## 中間変数の微分を表示させる方法


Chainerで、互いに依存する3つ以上の変数を作ってからbackward()を呼ぶと、中間変数の微分が表示されません。たとえば
```
import numpy as np
from chainer import Variable


x_data = np.array([5], dtype=np.float32)
x = Variable(x_data)
y = x**2 - 2 * x + 1
z = y*y
z.backward()


print(x.grad)
print(y.grad)
print(z.grad)
```

は、以下のようになります。

```
[ 256.]
None
[ 1.]
Program exited.
```

これは、中間変数の微分は深層学習の過程では通常は使われないからです。そこでChainerのデフォルト動作では、メモリを節約するために、backward()が終了後、中間変数の微分を消去してしまいます。
`retain_grad=True` を指定してやることで、中間変数の微分も保持されるようになります。



```
import numpy as np
from chainer import Variable


x_data = np.array([5], dtype=np.float32)
x = Variable(x_data)
y = x**2 - 2 * x + 1
a = 2*y
z = 3*y+2*a
z.backward(retain_grad=True)




print(x.grad)
print(y.grad)
print(a.grad)
print(z.grad)
```

を、実行すると、こうなります。

```
[ 56.]
[ 7.]
[ 2.]
[ 1.]
Program exited.
```

