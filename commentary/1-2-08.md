最適化によるパラメータの学習
公式bookの1/2/8

## メモ

================== 17.03.04 seki ==========================

最適化のフローチャート

1.　初期化（SGDやAdamなどを選択）

2.　学習対象のLinkをset up

3.　optimizer.updateでLinkのパラメータ最適化（例えば、L.LinearならばWとbが最適化。学習後、パラメータを確認すると確かに最適化されている）


=========================================


## 訂正 

================== 17.03.04 seki ==========================


説明文中頃のコード


```python
from chainer import optimizers

model = F.Classifier(MyLink())
optimizer = optimizers.Adam()
optimizer.use_cleargrads()
optimizer.setup(model)
```


MyLink定義していない。（あくまで、上記フローチャート1,2の一例）

=========================================


================== 17.03.04 seki ==========================


課題説明の最後の方

「・・・で与えられるデータセット(x,y)について最小二乗誤差（F.mean_squared_loss）を損失関数として使ってSGDで・・・」


誤

```python

F.mean_squared_loss

```

正

```python

F.mean_squared_error

```
=========================================



## 課題の解答例


================== 17.03.04 seki ==========================

```python

from chainer import optimizers                                                                                                      
from chainer import links as L
from chainer import Variable
from chainer import Chain
from chainer import functions as F
import numpy as np

class Linear(Chain):
    def __init__(self):
        super(Linear, self).__init__(
            l1=L.Linear(1, 1),
        )

    def __call__(self, x):
        return self.l1(x)


def f(x):
    return 5.*x - 10.

x = np.linspace(-10, 10, num=1001)
y = f(x) + 5.*np.random.randn(1001)

model = Linear()
                     
opt = optimizers.Adam()
opt.use_cleargrads()
opt.setup(model)     

for epoch in range(100):
# the number of learning
    perm = np.random.permutation(len(x))
# generating random permutation
    for i in range(len(x)):
        x_i = Variable(np.array([[x[perm[i]]]],'f'))
        y_i = Variable(np.array([[y[perm[i]]]],'f'))
# 'f' means "dtype=float32"
        opt.update(F.mean_squared_error, model.__call__(x_i), y_i)

print(model.l1.W.data)
print(model.l1.b.data)

```

=========================================




