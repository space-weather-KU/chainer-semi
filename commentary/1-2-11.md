
公式bookの1/2/11


次の節でちゃんと動かせるようになるので、いまは飛ばします。

## 課題の解体例　by seki

```python

import chainer                                                                                                                      
from chainer import datasets
from chainer import links as L
from chainer import functions as F
from chainer import Variable
import numpy as np

#============================= defining chain =============================
class MLP(chainer.Chain):

    def __init__(self, n_units1, n_units2, n_out):
        super(MLP, self).__init__(
# 「ニューラルネットワークの層」とは、ここの数のことを指す
            # the size of the inputs to each layer will be inferred
            l1=L.Linear(None, n_units1),  # n_in -> n_units1
            l2=L.Linear(None, n_units2),  # n_units1 -> n_units2
            l3=L.Linear(None, n_out),  # n_units2 -> n_out
# Linearの入力サイズがNoneとなっていますがこれは最初の実行時に入力から推論されます.
            )

    def __call__(self, x):
        h1 = F.relu(self.l1(x))
        h2 = F.relu(self.l2(h1))
        return self.l3(h2)
# relu returns the same value as the input unless it is negative. If the input is negative, return 0.
#=========================================================================

model = L.Classifier(MLP(784, 100, 10)) # , lossfun=***, accfun=$$$)
#    Classifier generates 3 functions.
#        predictor 　学習対象であるLink
#        lossfun 　誤差関数に使う関数。default : softmax_cross_entropy
#        accfun 　精度評価につかう関数。default : accuracy

opt = chainer.optimizers.Adam()
opt.use_cleargrads()
opt.setup(model)


#======================== training ========================================
batchsize = 500
train, test = chainer.datasets.get_mnist()
# getting the MNIST dataset and saving the numpy array.
train_iter = chainer.iterators.SerialIterator(train, batch_size=batchsize, shuffle=True)
test_iter = chainer.iterators.SerialIterator(test, batch_size=batchsize, repeat=False, shuffle=False)
# defining how to iterate over these datasets
#==========================================================================


train_num = len(train)
for i in range(0, train_num, batchsize):
    batch = train_iter.next()
# selecting the random 500 (tuple) images
    x = Variable(np.array([ss[0] for ss in batch]))
    t = Variable(np.array([ss[1] for ss in batch]))
# np.asarray() converts the input to an array.
# "for ss in batch" makes ss array.
    opt.update(model.lossfun, model.predictor(x), t)
    print(model.accfun(model.predictor(x), t).data)

```




